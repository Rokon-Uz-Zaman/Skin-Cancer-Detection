{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Cleaning Data:\n",
    "\n",
    "-making Transforms for preprocessing\n",
    "-specifing the dataset and their diretory, then applying the transforms on them.\n",
    "-passing transforms to data loaders that pack the datasets into batches\n",
    "'''\n",
    "data_dir = 'path_for_dataset'\n",
    "\n",
    "# TODO: Define transforms for the training data and testing data\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "valid_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder(data_dir + '/valid', transform=test_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Using the pretrained model on Imagenet Dataset(a huge dataset with 1000 different classes)\n",
    "then turning off their parameters for training(they won't be pdated if trained)\n",
    "then removing the classifier layers(the fully connected layers)\n",
    "finally, adding our new classifier to the network which has 3 outputs only not 1000\n",
    "'''\n",
    "model= models.resnet152(pretrained = True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(2048, 512)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('Dropout',nn.Dropout(0.45)),\n",
    "                          ('fc2', nn.Linear(512, 3)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "\n",
    "model.fc = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "using Adam as an optimiser as it has a momentum, we don't want to be stuck in a local minima.\n",
    "adn using Negative Likelihood Log Loss function for the criterion. note that the finall layer is log softmax\n",
    "\n",
    "'''\n",
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('temp_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.587527 \tValidation Loss: 0.684956\n",
      "Epoch: 2 \tTraining Loss: 0.580916 \tValidation Loss: 0.695845\n",
      "Epoch: 3 \tTraining Loss: 0.570567 \tValidation Loss: 0.691252\n",
      "Epoch: 4 \tTraining Loss: 0.584392 \tValidation Loss: 0.748235\n",
      "Epoch: 5 \tTraining Loss: 0.587244 \tValidation Loss: 0.702227\n",
      "Epoch: 6 \tTraining Loss: 0.579297 \tValidation Loss: 0.684876\n",
      "Epoch: 7 \tTraining Loss: 0.570376 \tValidation Loss: 0.715707\n",
      "Epoch: 8 \tTraining Loss: 0.585352 \tValidation Loss: 0.711866\n",
      "Epoch: 9 \tTraining Loss: 0.563048 \tValidation Loss: 0.661966\n",
      "Validation loss decreased (0.669739 --> 0.661966).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.576070 \tValidation Loss: 0.682396\n",
      "Epoch: 11 \tTraining Loss: 0.569796 \tValidation Loss: 0.677349\n",
      "Epoch: 12 \tTraining Loss: 0.568266 \tValidation Loss: 0.672753\n",
      "Epoch: 13 \tTraining Loss: 0.588864 \tValidation Loss: 0.731330\n",
      "Epoch: 14 \tTraining Loss: 0.586311 \tValidation Loss: 0.656463\n",
      "Validation loss decreased (0.661966 --> 0.656463).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.601241 \tValidation Loss: 0.656417\n",
      "Validation loss decreased (0.656463 --> 0.656417).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.581053 \tValidation Loss: 0.651276\n",
      "Validation loss decreased (0.656417 --> 0.651276).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.579213 \tValidation Loss: 0.658413\n",
      "Epoch: 18 \tTraining Loss: 0.580512 \tValidation Loss: 0.673474\n",
      "Epoch: 19 \tTraining Loss: 0.574700 \tValidation Loss: 0.671414\n",
      "Epoch: 20 \tTraining Loss: 0.559984 \tValidation Loss: 0.683758\n",
      "Epoch: 21 \tTraining Loss: 0.571950 \tValidation Loss: 0.667945\n",
      "Epoch: 22 \tTraining Loss: 0.565466 \tValidation Loss: 0.663354\n",
      "Epoch: 23 \tTraining Loss: 0.544240 \tValidation Loss: 0.657886\n",
      "Epoch: 24 \tTraining Loss: 0.575006 \tValidation Loss: 0.661749\n",
      "Epoch: 25 \tTraining Loss: 0.562198 \tValidation Loss: 0.653689\n",
      "Epoch: 26 \tTraining Loss: 0.574268 \tValidation Loss: 0.688936\n",
      "Epoch: 27 \tTraining Loss: 0.573248 \tValidation Loss: 0.717708\n",
      "Epoch: 28 \tTraining Loss: 0.564111 \tValidation Loss: 0.681586\n",
      "Epoch: 29 \tTraining Loss: 0.565491 \tValidation Loss: 0.669578\n",
      "Epoch: 30 \tTraining Loss: 0.581656 \tValidation Loss: 0.685135\n"
     ]
    }
   ],
   "source": [
    "## number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "#valid_loss_min = np.Inf # track change in validation loss\n",
    "#valid_loss_min =0.6697391470273336\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "\n",
    "    torch.save(model.state_dict(), \"temp_model.pt\")  \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "       # model_name=  \"model_with_loss=_\"+str(valid_loss)+\".pt\"\n",
    "        torch.save(model.state_dict(), 'model_dermatology.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_dermatology.pt'))\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch index 0\n",
      "batch index 1\n",
      "batch index 2\n",
      "batch index 3\n",
      "batch index 4\n",
      "batch index 5\n",
      "Test Loss: 0.660876\n",
      "\n",
      "\n",
      "Test Accuracy (Overall): 72% (435/600)\n"
     ]
    }
   ],
   "source": [
    "classes=[\"melanoma\",\"nevus\",\"seborrhic keratosis\"]\n",
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(3))\n",
    "class_total = list(0. for i in range(3))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(100):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "    print(\"batch index\",batch_idx)\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
